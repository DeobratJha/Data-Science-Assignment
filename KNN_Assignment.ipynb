{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7Fdwt0MWHzZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the KNN algorithms?\n",
        "\n",
        "Ans.\n",
        "\n",
        "*   K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique\n",
        "*   K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
        "*   K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
        "*   K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n",
        "*  It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pj5qxkq3WXz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. How do you choose the value of K in KNN?\n",
        "Ans.\n",
        " Below are some points to remember while selecting the value of K in the K-NN algorithm:\n",
        "*  There is no particular way to determine the best value for \"K\", so we need to try some values to find the best out of them. The most preferred value for K is 5.\n",
        "*   A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.\n",
        "*  Large values for K are good, but it may find some difficulties.\n",
        "\n"
      ],
      "metadata": {
        "id": "UT6aduSlXjs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "Ans.\n",
        "knn classification tasks, the user seeks to predict a category, which is usually represented as an integer label, but represents a category of \"things\". For instance, you could try to classify pictures between \"cat\" and \"dog\" and use label 0 for \"cat\" and 1 for \"dog\".\n",
        "\n",
        "The KNN algorithm for classification will look at the k nearest neighbours of the input you are trying to make a prediction on. It will then output the most frequent label among those k examples.\n",
        "\n",
        "In regression tasks, the user wants to output a numerical value (usually continuous). It may be for instance estimate the price of a house, or give an evaluation of how good a movie is.\n",
        "\n",
        "In this case, the KNN algorithm would collect the values associated with the k closest examples from the one you want to make a prediction on and aggregate them to output a single value."
      ],
      "metadata": {
        "id": "wvDKHgEGYftK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. How do you measure the performance of KNN?\n",
        "\n",
        "Ans.The optimal K value usually found is the square root of N, where N is the total number of samples. Use an error plot or accuracy plot to find the most favorable K value. KNN performs well with multi-label classes, but you must be aware of the outliers."
      ],
      "metadata": {
        "id": "YGyi7f5kaMnl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "Ans.The k-nearest neighbors algorithm hinges on data points being close together. This becomes challenging as the number of dimensions increases, referred to as the “Curse of Dimensionality.” It’s especially hard for the k-nearest neighbors algorithm it requires two points to be very close on every axis, and adding a new dimension creates another opportunity for points to be farther apart. As the number of dimensions increases, the closest distance between two points approaches the average distance between points, eradicating the ability of the k-nearest neighbors algorithm to provide valuable predictions.\n",
        "\n",
        "To overcome this challenge, you can add more data to the data set. By doing so you add density to the data space, bringing the nearest points closer together and returning the ability of the k-nearest neighbors algorithm to provide valuable predictions. This is a valuable solution so long as you have the hardware needed to perform computations on your data set. As your data set gets larger and larger, you need more and more computing power to process it. Eventually the size of your data set will surpass your computing power. At that point, you need to use dimensionality reduction to present all of the valuable information in fewer dimensions."
      ],
      "metadata": {
        "id": "9q32FPuHbDUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How do you handle missing values in KNN?\n",
        "\n",
        "Ans.\n",
        "It is common to identify missing values in a dataset and replace them with a numeric value. This is called data imputing, or missing data imputation.\n",
        "\n",
        "*   Removal of instances: One simple approach is to remove instances that contain missing values. However, this may result in significant data loss, especially if the missing values are prevalent in the dataset.\n",
        "*   Imputation: Another approach is to impute or fill in the missing values with estimated values. There are various imputation techniques that can be used, such as:\n",
        "\n",
        "a. Mean imputation: Replace missing values with the mean value of the feature across all instances. This approach assumes that the missing values are missing completely at random and does not consider any relationships between features.\n",
        "\n",
        "b. Median imputation: Similar to mean imputation, but the missing values are replaced with the median value of the feature. This approach is less sensitive to outliers compared to mean imputation.\n",
        "\n",
        "c. Mode imputation: Used for categorical features, where missing values are replaced with the most frequent value (mode) of the feature.\n",
        "\n",
        "d. Regression imputation: Create a regression model using other features as predictors and use it to predict the missing values. This method takes into account the relationships between features but assumes linearity.\n",
        "\n",
        "e. KNN imputation: Use KNN to impute missing values. In this approach, the missing value is estimated based on the average value of its K nearest neighbors.\n",
        "\n",
        "*   Treat missing values as a separate category: For categorical features, you can consider treating missing values as a separate category. This means creating a new category to represent missing values and keeping them as a distinct class.\n",
        "\n",
        "It is important to note that the choice of the imputation method depends on the specific dataset and the nature of missing values. Care should be taken to avoid introducing bias or distorting the original data distribution while handling missing values in KNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "3bZAi_LzcCTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
        "which type of problem?\n",
        "\n",
        "ANS.\n",
        "\n",
        "KNN classifier and KNN regressor are both variants of the K-nearest neighbors algorithm, but they are used for different types of problems and have distinct performance characteristics:\n",
        "\n",
        "KNN Classifier:\n",
        "\n",
        "KNN classifier is used for classification problems where the goal is to assign a class label to a given input.\n",
        "It predicts the class of an instance by majority voting among its K nearest neighbors.\n",
        "KNN classifier works well with categorical or discrete target variables.\n",
        "It does not make any assumptions about the underlying data distribution.\n",
        "The performance of the KNN classifier can be affected by the choice of K, distance metric, and the presence of irrelevant features.\n",
        "It can handle multi-class classification problems.\n",
        "KNN classifier may suffer from the curse of dimensionality when the number of features is large, leading to increased computational complexity.\n",
        "\n",
        "KNN Regressor:\n",
        "\n",
        "KNN regressor is used for regression problems where the goal is to predict a continuous numeric value for a given input.\n",
        "It predicts the output by averaging the target values of its K nearest neighbors.\n",
        "KNN regressor is suitable for problems where the target variable is continuous or numerical.\n",
        "It does not make any assumptions about the functional form of the relationship between the features and the target variable.\n",
        "The performance of the KNN regressor can be influenced by the choice of K, distance metric, and the presence of outliers.\n",
        "KNN regressor can handle multi-dimensional output variables.\n",
        "Similar to the KNN classifier, the curse of dimensionality can affect the performance of the KNN regressor when dealing with high-dimensional data.\n",
        "Choosing between KNN classifier and KNN regressor depends on the nature of the problem at hand:\n",
        "\n",
        "For classification problems where the target variable is categorical, KNN classifier is typically more suitable.\n",
        "For regression problems where the target variable is continuous or numerical, KNN regressor is more appropriate.\n",
        "If the target variable falls within a discrete range, it may be possible to convert a regression problem into a classification problem by binning the target values and using KNN classifier instead.\n",
        "It's worth noting that the performance of both KNN classifier and KNN regressor can be influenced by the choice of hyperparameters, feature scaling, and the quality and relevance of the training data.\n",
        "In summary, the choice between KNN classifier and KNN regressor depends on the type of problem and the nature of the target variable. Understanding the problem requirements and characteristics of the data will guide the selection of the appropriate variant of the KNN algorithm."
      ],
      "metadata": {
        "id": "2fC4O4zUeLfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
        "and how can these be addressed?\n",
        "\n",
        "Ans.\n",
        "\n",
        "Strengths of KNN:\n",
        "\n",
        "\n",
        "*   Intuitive and simple: KNN is easy to understand and implement, making it a popular choice for beginners. It does not make any assumptions about the underlying data distribution.\n",
        "*  Non-parametric: KNN is a non-parametric algorithm, meaning it does not assume any specific functional form for the relationship between features and target variables. It can capture complex patterns and adapt to different types of data.\n",
        "\n",
        "\n",
        "*   Versatility: KNN can handle both classification and regression tasks. It can be applied to problems with categorical or continuous target variables.\n",
        "\n",
        "Weaknesses of KNN:\n",
        "\n",
        "\n",
        "\n",
        "*   Computationally expensive: KNN requires computing distances between the query instance and all the training instances. As the dataset grows, the computational cost increases significantly, especially when using large values of K or high-dimensional data.\n",
        "*   Sensitivity to feature scaling: KNN is sensitive to the scale and range of features. Features with larger scales can dominate the distance calculations, leading to biased results. It is important to normalize or scale the features appropriately before applying KNN.\n",
        "\n",
        "*  Curse of dimensionality: KNN's performance can degrade when dealing with high-dimensional data. In high-dimensional spaces, the notion of distance becomes less reliable due to the sparsity of data, leading to decreased effectiveness of nearest neighbors.\n",
        "*  Imbalanced data: KNN can be biased towards the majority class in imbalanced datasets since the majority class tends to have more nearby neighbors. This can lead to poor performance in predicting the minority class.\n",
        "\n",
        "\n",
        "Addressing the weaknesses of KNN:\n",
        "\n",
        "\n",
        "*   Feature scaling: To address sensitivity to feature scaling, it is important to normalize or scale the features so that each feature contributes proportionately to the distance calculations. Techniques such as min-max scaling or standardization (mean normalization) can be used.\n",
        "\n",
        "*  Dimensionality reduction: When dealing with high-dimensional data, dimensionality reduction techniques like principal component analysis (PCA) or feature selection methods can be applied to reduce the number of features while retaining important information.\n",
        "*   Distance metrics: Choosing an appropriate distance metric is crucial. The Euclidean distance is commonly used, but for specific data types or structures, other distance measures like Manhattan distance or cosine similarity may be more suitable.\n",
        "\n",
        "\n",
        "*   Distance metrics: Choosing an appropriate distance metric is crucial. The Euclidean distance is commonly used, but for specific data types or structures, other distance measures like Manhattan distance or cosine similarity may be more suitable.\n",
        "\n",
        "*   Handling imbalanced data: Techniques such as oversampling the minority class, undersampling the majority class, or using synthetic data generation methods like SMOTE (Synthetic Minority Over-sampling Technique) can help address imbalanced data issues.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FB_dHX7CfOI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "\n",
        "ANS.\n",
        "\n",
        "Euclidean distance and Manhattan distance are two commonly used distance metrics in the K-nearest neighbors (KNN) algorithm. They differ in the way they measure the distance between two points in a multi-dimensional space:\n",
        "\n",
        "Euclidean Distance:\n",
        "\n",
        "Euclidean distance is the straight-line or Euclidean distance between two points in a Cartesian plane.\n",
        "It is calculated as the square root of the sum of the squared differences between corresponding coordinates of the two points.\n",
        "In a 2-dimensional space, the Euclidean distance between points (x1, y1) and (x2, y2) is given by: sqrt((x2 - x1)^2 + (y2 - y1)^2).\n",
        "Euclidean distance considers the actual geometric distance between points, giving importance to both horizontal and vertical differences.\n",
        "It assumes that all dimensions (features) have equal importance and contributes equally to the overall distance calculation.\n",
        "Euclidean distance is sensitive to the scale of features, meaning that features with larger scales can dominate the distance calculation.\n",
        "Manhattan Distance:\n",
        "\n",
        "Manhattan distance, also known as city block distance or L1 norm, measures the distance between two points by summing the absolute differences between their coordinates.\n",
        "It is calculated as the sum of the absolute differences between corresponding coordinates of the two points.\n",
        "In a 2-dimensional space, the Manhattan distance between points (x1, y1) and (x2, y2) is given by: |x2 - x1| + |y2 - y1|.\n",
        "Manhattan distance measures the distance in terms of the number of blocks or steps required to move from one point to another in a city-like grid.\n",
        "It considers only the horizontal and vertical differences between points, ignoring diagonal movements.\n",
        "Manhattan distance is less sensitive to the scale of features since it only considers the absolute differences between coordinates."
      ],
      "metadata": {
        "id": "f5GIZFvLhRUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What is the role of feature scaling in KNN?\n",
        "\n",
        "ANS.\n",
        "\n",
        "Feature scaling plays a crucial role in the K-nearest neighbors (KNN) algorithm. It is the process of normalizing or standardizing the features of the dataset to ensure that they have a similar scale or range. The main role of feature scaling in KNN is to ensure that all features contribute equally to the distance calculations between data points. Here's why feature scaling is important in KNN:\n",
        "\n",
        "Distance-based calculations: KNN algorithm relies on calculating distances between data points to determine the nearest neighbors. The distance metric, such as Euclidean or Manhattan distance, quantifies the similarity or dissimilarity between instances. If the features have different scales, those with larger values can dominate the distance calculations, leading to biased results. Feature scaling helps to avoid this issue by bringing all features to a similar scale.\n",
        "\n",
        "Equalizing feature contributions: In KNN, each feature contributes to the overall distance calculation. However, features with larger scales can have a disproportionately larger impact on the distance. Scaling the features ensures that they have a similar influence on the distance computation, preventing one feature from overshadowing others.\n",
        "\n",
        "Avoiding dimensionality bias: Feature scaling helps in situations where the scale of one feature is significantly larger than others. In high-dimensional spaces, such as datasets with many features, this becomes even more crucial. Without feature scaling, the KNN algorithm might be biased towards features with larger scales, potentially ignoring the information carried by features with smaller scales.\n",
        "\n",
        "Normalizing the data distribution: Scaling features can help in normalizing the distribution of the dataset, which can be beneficial for certain distance metrics or algorithms. It can reduce the impact of outliers and make the data more suitable for KNN.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Roc-K3gh1M7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uqwBi-hxWoAl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}